{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DIR = r'C:\\Users\\MCC\\Desktop\\landmark\\trainset'\n",
    "# train_folder_list=array(os.listdir(TRAIN_DIR))\n",
    "\n",
    "# train_input = []\n",
    "# train_label = []\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(train_folder_list)\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# for index in range(len(train_folder_list)):\n",
    "#     path = os.path.join(TRAIN_DIR, train_folder_list[index])\n",
    "#     img_list = os.listdir(path)\n",
    "#     for img in img_list:\n",
    "#         img_path = os.path.join(path, img)\n",
    "#         img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "#         train_input.append([np.array(img)])\n",
    "#         train_label.append([np.array(onehot_encoded[index])])\n",
    "        \n",
    "# train_input = np.reshape(train_input, (-1,3518976))\n",
    "# train_label = np.reshape(train_label, (-1,5))\n",
    "# train_input = np.array(train_input).astype(np.float32)\n",
    "# train_label = np.array(train_label).astype(np.float32)\n",
    "# np.save(\"train_data.npy\", train_input)\n",
    "# np.save(\"train_label.npy\", train_label)\n",
    "train_input = np.load(r'C:\\Users\\MCC\\Documents\\jupyter\\train_data.npy')\n",
    "train_label = np.load(r'C:\\Users\\MCC\\Documents\\jupyter\\train_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DIR = r'C:\\Users\\MCC\\Desktop\\landmark\\testset'\n",
    "# test_folder_list=array(os.listdir(TEST_DIR))\n",
    "\n",
    "# test_input = []\n",
    "# test_label = []\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(test_folder_list)\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "# for index in range(len(test_folder_list)):\n",
    "#     path = os.path.join(TEST_DIR, test_folder_list[index])\n",
    "#     img_list = os.listdir(path)\n",
    "#     for img in img_list:\n",
    "#         img_path = os.path.join(path, img)\n",
    "#         img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "#         test_input.append([np.array(img)])\n",
    "#         test_label.append([np.array(onehot_encoded[index])])\n",
    "        \n",
    "# test_input = np.reshape(test_input, (-1,3518976))\n",
    "# test_label = np.reshape(test_label, (-1,5))\n",
    "# test_input = np.array(test_input).astype(np.float32)\n",
    "# test_label = np.array(test_label).astype(np.float32)\n",
    "# np.save(\"test_data.npy\", test_input)\n",
    "# np.save(\"test_label.npy\", test_label)\n",
    "test_input = np.load(r'C:\\Users\\MCC\\Documents\\jupyter\\test_data.npy')\n",
    "test_label = np.load(r'C:\\Users\\MCC\\Documents\\jupyter\\test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 3518976])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 5])\n",
    "\n",
    "x_image =tf.reshape(x, [-1, 1264, 928, 3])\n",
    "x_test =tf.reshape(x, [-1, 1264, 928, 3])\n",
    "\n",
    "trainphase = tf.placeholder(tf.bool)\n",
    "keepprob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, in_ch, out_ch, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        w_conv =tf.get_variable(name='weights', shape=[3, 3, in_ch, out_ch], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        h_bn = tf.layers.batch_normalization(tf.nn.conv2d(x, w_conv, strides=[1, 1, 1, 1], padding='SAME'), training=trainphase)\n",
    "        h_conv = tf.nn.relu(h_bn)\n",
    "    return h_conv\n",
    "\n",
    "with tf.variable_scope(\"block1\") as scope:\n",
    "    h_conv1 = conv(x_image, 3, 2, \"Conv1\")\n",
    "    h_conv2 = conv(h_conv1, 2, 2, \"Conv2\")\n",
    "    h_conv2_pool = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.variable_scope(\"block2\") as scope:\n",
    "    h_conv3 = conv(h_conv2_pool, 2, 4, \"Conv3\")\n",
    "    h_conv4 = conv(h_conv3, 4, 4, \"Conv4\")\n",
    "    h_conv4_pool = tf.nn.max_pool(h_conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.variable_scope(\"block3\") as scope:\n",
    "    h_conv5 = conv(h_conv4_pool, 4, 8, \"Conv5\")\n",
    "    h_conv6 = conv(h_conv5, 8, 8, \"Conv6\")\n",
    "    h_conv6_pool = tf.nn.max_pool(h_conv6, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.variable_scope(\"block4\") as scope:\n",
    "    h_conv7 = conv(h_conv6_pool, 8, 16, \"Conv7\")\n",
    "    h_conv8 = conv(h_conv7, 16, 16, \"Conv8\")\n",
    "    h_conv8_pool = tf.nn.max_pool(h_conv8, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with tf.variable_scope(\"block5\") as scope:\n",
    "    h_conv9 = conv(h_conv8_pool, 16, 32, \"Conv9\")\n",
    "    h_conv10 = conv(h_conv9, 32, 32, \"Conv10\")\n",
    "    h_conv10_pool = tf.nn.max_pool(h_conv10, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')    \n",
    "\n",
    "with tf.variable_scope(\"block7\") as scope:\n",
    "    h_conv11 = conv(h_conv10_pool, 32, 64, \"Conv11\")\n",
    "    h_conv12 = conv(h_conv11, 64, 64, \"Conv12\")\n",
    "    h_conv12_pool = tf.nn.max_pool(h_conv12, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')        \n",
    "\n",
    "with tf.variable_scope(\"block8\") as scope:\n",
    "    h_conv13 = conv(h_conv12_pool, 64, 128, \"Conv13\")\n",
    "    h_conv14 = conv(h_conv13, 128, 128, \"Conv14\")\n",
    "    h_conv14_pool = tf.nn.max_pool(h_conv14, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')        \n",
    "    \n",
    "with tf.variable_scope(\"block9\") as scope:\n",
    "    h_conv15 = conv(h_conv14_pool, 128, 256, \"Conv15\")\n",
    "    h_conv16 = conv(h_conv15, 256, 256, \"Conv16\")\n",
    "    h_conv16_pool = tf.nn.max_pool(h_conv16, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')        \n",
    "\n",
    "with tf.variable_scope(\"block10\") as scope:\n",
    "    h_conv17 = conv(h_conv16_pool, 256, 512, \"Conv17\")\n",
    "    h_conv18 = conv(h_conv17, 512, 512, \"Conv18\")\n",
    "    h_conv18_pool = tf.nn.max_pool(h_conv18, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')        \n",
    "    \n",
    "with tf.variable_scope(\"block11\") as scope:\n",
    "    h_conv19 = conv(h_conv18_pool, 512, 1024, \"Conv19\")\n",
    "    h_conv20 = conv(h_conv19, 1024, 1024, \"Conv20\")\n",
    "    h_conv20_pool = tf.nn.max_pool(h_conv20, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')        \n",
    "\n",
    "with tf.variable_scope(\"block12\") as scope:\n",
    "    h_conv21 = conv(h_conv20_pool, 1024, 2048, \"Conv15\")\n",
    "    h_conv22 = conv(h_conv21, 2048, 2048, \"Conv16\")\n",
    "    h_conv22_pool = tf.nn.max_pool(h_conv22, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')            \n",
    "    \n",
    "with tf.variable_scope(\"fclayer\") as scope:\n",
    "    h_conv22_pool_flat = tf.reshape(h_conv22_pool, [-1, 1*1*2048])\n",
    "    h_conv22_pool_flat_dropout = tf.nn.dropout(h_conv22_pool_flat, keep_prob=keepprob)\n",
    "    w_fc = tf.get_variable(name='weights', shape=[1*1*2048, 5], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fc = tf.Variable(tf.constant(0.1, shape=[5]))\n",
    "    logits = tf.matmul(h_conv22_pool_flat_dropout, w_fc) + b_fc\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "    \n",
    "with tf.name_scope(\"l2_loss\") as scope:\n",
    "    vars = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars]) * 0.0005\n",
    "    \n",
    "with tf.name_scope(\"trainer\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)) + lossL2\n",
    "    train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"evaluation\") as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1,), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(epcoh):  0 트레이닝 데이터 정확도:  0.5815 손실함수:  121013975000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  1 트레이닝 데이터 정확도:  0.075 손실함수:  203849520000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  2 트레이닝 데이터 정확도:  0.0 손실함수:  79159620000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  3 트레이닝 데이터 정확도:  0.0 손실함수:  113592305000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  4 트레이닝 데이터 정확도:  0.0 손실함수:  125641800000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  5 트레이닝 데이터 정확도:  0.0009999999 손실함수:  141025720000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  6 트레이닝 데이터 정확도:  0.007999999 손실함수:  118401800000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  7 트레이닝 데이터 정확도:  0.011499999 손실함수:  136149910000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  8 트레이닝 데이터 정확도:  0.024 손실함수:  173761480000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  9 트레이닝 데이터 정확도:  0.038 손실함수:  187655930000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  10 트레이닝 데이터 정확도:  0.042999998 손실함수:  171882610000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  11 트레이닝 데이터 정확도:  0.05 손실함수:  113812610000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  12 트레이닝 데이터 정확도:  0.0655 손실함수:  112960950000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  13 트레이닝 데이터 정확도:  0.035 손실함수:  33491406000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  14 트레이닝 데이터 정확도:  0.0455 손실함수:  45593400000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  15 트레이닝 데이터 정확도:  0.051999997 손실함수:  72117880000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  16 트레이닝 데이터 정확도:  0.053499997 손실함수:  75790860000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  17 트레이닝 데이터 정확도:  0.060500003 손실함수:  95606090000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  18 트레이닝 데이터 정확도:  0.076 손실함수:  80768050000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  19 트레이닝 데이터 정확도:  0.07149999 손실함수:  116163320000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  20 트레이닝 데이터 정확도:  0.09949999 손실함수:  62441490000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  21 트레이닝 데이터 정확도:  0.1005 손실함수:  144944540000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  22 트레이닝 데이터 정확도:  0.1 손실함수:  120037625000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  23 트레이닝 데이터 정확도:  0.0215 손실함수:  252356880000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  24 트레이닝 데이터 정확도:  0.050499998 손실함수:  242168610000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  25 트레이닝 데이터 정확도:  0.0815 손실함수:  207676030000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  26 트레이닝 데이터 정확도:  0.088 손실함수:  188278580000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  27 트레이닝 데이터 정확도:  0.09199999 손실함수:  124068170000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  28 트레이닝 데이터 정확도:  0.1055 손실함수:  218624440000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  29 트레이닝 데이터 정확도:  0.10849999 손실함수:  244419370000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  30 트레이닝 데이터 정확도:  0.09700001 손실함수:  167265490000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  31 트레이닝 데이터 정확도:  0.1025 손실함수:  115497620000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  32 트레이닝 데이터 정확도:  0.1485 손실함수:  167457790000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  33 트레이닝 데이터 정확도:  0.1355 손실함수:  226677100000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  34 트레이닝 데이터 정확도:  0.1925 손실함수:  166495800000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  35 트레이닝 데이터 정확도:  0.020499999 손실함수:  755750600000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  36 트레이닝 데이터 정확도:  0.009500001 손실함수:  323745200000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  37 트레이닝 데이터 정확도:  0.022999998 손실함수:  317618300000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  38 트레이닝 데이터 정확도:  0.089999996 손실함수:  378409570000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  39 트레이닝 데이터 정확도:  0.14049998 손실함수:  379053450000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  40 트레이닝 데이터 정확도:  0.16 손실함수:  334081250000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  41 트레이닝 데이터 정확도:  0.19799998 손실함수:  376815670000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  42 트레이닝 데이터 정확도:  0.225 손실함수:  329071600000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  43 트레이닝 데이터 정확도:  0.23300001 손실함수:  464506350000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  44 트레이닝 데이터 정확도:  0.16800001 손실함수:  412723800000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  45 트레이닝 데이터 정확도:  0.135 손실함수:  749629200000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  46 트레이닝 데이터 정확도:  0.0565 손실함수:  248023930000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  47 트레이닝 데이터 정확도:  0.0645 손실함수:  931373700000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  48 트레이닝 데이터 정확도:  0.0945 손실함수:  115910650000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  49 트레이닝 데이터 정확도:  0.1605 손실함수:  224671990000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  50 트레이닝 데이터 정확도:  0.22449999 손실함수:  258302660000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  51 트레이닝 데이터 정확도:  0.25349998 손실함수:  236126030000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  52 트레이닝 데이터 정확도:  0.3105 손실함수:  335904360000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  53 트레이닝 데이터 정확도:  0.2585 손실함수:  306118220000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  54 트레이닝 데이터 정확도:  0.31149998 손실함수:  336610650000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  55 트레이닝 데이터 정확도:  0.303 손실함수:  515056340000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  56 트레이닝 데이터 정확도:  0.3215 손실함수:  718342600000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  57 트레이닝 데이터 정확도:  0.3005 손실함수:  1083648400000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  58 트레이닝 데이터 정확도:  0.32849997 손실함수:  1149859700000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  59 트레이닝 데이터 정확도:  0.378 손실함수:  1367419000000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  60 트레이닝 데이터 정확도:  0.39650002 손실함수:  1014975500000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  61 트레이닝 데이터 정확도:  0.43450004 손실함수:  1881303700000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  62 트레이닝 데이터 정확도:  0.466 손실함수:  1802134600000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  63 트레이닝 데이터 정확도:  0.4815 손실함수:  1779634900000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  64 트레이닝 데이터 정확도:  0.48149997 손실함수:  2300002000000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  65 트레이닝 데이터 정확도:  0.502 손실함수:  1676749700000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  66 트레이닝 데이터 정확도:  0.50299996 손실함수:  1285411100000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  67 트레이닝 데이터 정확도:  0.541 손실함수:  1842832000000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  68 트레이닝 데이터 정확도:  0.586 손실함수:  1519899500000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  69 트레이닝 데이터 정확도:  0.6225 손실함수:  306591580000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  70 트레이닝 데이터 정확도:  0.6065 손실함수:  540742300000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  71 트레이닝 데이터 정확도:  0.5445 손실함수:  2558446300000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  72 트레이닝 데이터 정확도:  0.538 손실함수:  3944745700000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  73 트레이닝 데이터 정확도:  0.59499997 손실함수:  1387955900000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  74 트레이닝 데이터 정확도:  0.60150003 손실함수:  1446774400000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  75 트레이닝 데이터 정확도:  0.64350003 손실함수:  1621677200000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  76 트레이닝 데이터 정확도:  0.642 손실함수:  3583817000000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  77 트레이닝 데이터 정확도:  0.6365 손실함수:  2511181000000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  78 트레이닝 데이터 정확도:  0.654 손실함수:  1561686100000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  79 트레이닝 데이터 정확도:  0.65749997 손실함수:  1670718300000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n",
      "반복(epcoh):  80 트레이닝 데이터 정확도:  0.6595 손실함수:  1393993100000000.0\n",
      "테스트 데이터 정확도:  nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_epoch = 1000\n",
    "    for e in range(total_epoch):\n",
    "        total_size = train_input.shape[0]\n",
    "        batch_size = 50\n",
    "        loss_list = []\n",
    "        train_accuracy_list = []\n",
    "        for i in range(int(total_size / batch_size)):\n",
    "            batch_x = train_input[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = train_label[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            sess.run(train_step, feed_dict={x: batch_x, y: batch_y, trainphase: True, keepprob: 0.7})\n",
    "            \n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y, trainphase: True, keepprob: 1})\n",
    "            loss_print = loss.eval(feed_dict={x: batch_x, y: batch_y, trainphase: False, keepprob: 1})\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            loss_list.append(loss_print)\n",
    "        print(\"반복(epcoh): \",e,\"트레이닝 데이터 정확도: \",np.mean(train_accuracy_list), \"손실함수: \", np.mean(loss_list))\n",
    "        \n",
    "        test_total_size = test_input.shape[0]\n",
    "        test_batch_size = 5\n",
    "    \n",
    "        test_accuracy_list = []\n",
    "        for i in range(int(test_total_size/ test_batch_size)):\n",
    "            test_batch_x = test_input[i*batch_size:(i+1)*batch_size]\n",
    "            test_batch_y = test_label[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "            test_accuracy = accuracy.eval(feed_dict={x:test_batch_x, y: test_batch_y, trainphase: False, keepprob: 1})\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "        print(\"테스트 데이터 정확도: \", np.mean(test_accuracy_list))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_total_size = test_input.shape[0]\n",
    "    test_batch_size = 5\n",
    "    \n",
    "    test_accuracy_list = []\n",
    "    for i in range(int(test_total_size/ test_batch_size)):\n",
    "        test_batch_x = test_input[i*batch_size:(i+1)*batch_size]\n",
    "        test_batch_y = test_label[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        test_accuracy = accuracy.eval(feed_dict={x:test_batch_x, y: test_batch_y, trainphase: False, keepprob: 1})\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "    print(\"테스트 데이터 정확도: \", np.mean(test_accuracy_list))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
